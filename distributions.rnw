\documentclass[12pt]{article}  
\usepackage[linesnumbered]{algorithm2e} \usepackage{amsmath} \usepackage{amsthm} \usepackage{amsfonts} \usepackage{bbm} \usepackage{color,soul} \usepackage{framed} \usepackage[margin=0.5in]{geometry} \usepackage{hyperref} \usepackage{mathtools} \usepackage[dvipsnames]{xcolor} 
\usepackage{tikz} \newtheorem{theorem}{Theorem}[section] \newtheorem{lemma}[theorem]{Lemma} \newtheorem{proposition}[theorem]{Proposition} \newtheorem{corollary}[theorem]{Corollary}  \DeclarePairedDelimiter{\ceil}{\lceil}{\rceil} \DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor} \DeclareMathOperator*{\argmin}{arg\,min} \DeclareMathOperator*{\argmax}{arg\,max} \newcommand{\D}{\mathrm{d}} \SetKwInput{KwInput}{Input} \SetKwInput{KwOutput}{Output}  \begin{document}

\title{\textbf{Tenants of Probability}}
\author{Andreas Santucci}
\date{August 2017}
\maketitle

\section{Introduction to Probability}
Basic concepts of probability are more easily explained in a finite state space.
There is a finite set
\[
\mathcal S = \{s_1, \ldots, s_M\}
\]
whose elements are are said to be \emph{elementary events}. 
A \emph{probability measure} assigns a real number $P(s)$ to each state $s \in S$.
One and only one of the states will occur, and that's reflected in the probabilities being
non-negative and summing to one.

\begin{align*}   \Pr(s_j) \geq 0, \hspace{25pt} \sum_{j=1}^M \Pr(s_j) = 1. \end{align*}

An \emph{event} $A$ is a subset of state-space $S$, and it is said to occur if it contains the state that occurs. I.e.
\[
\Pr(A) = \sum_{s \in A} \Pr(s).
\]

We take as axiom the following rule of conditional probability, 
$\Pr(A,B) = \Pr(A) \cdot \Pr(B | A)$.
A \emph{random variable} is a mapping from the state-space to the real-numbers. Its expectation is 
formed by a weighted average,
\[
\mathbb E[Y] = \sum_{j=1}^{M} Y(s_j) \Pr(s_j).
\]

We have the \emph{law of total probability},
\[
\Pr(A) = \sum_{j} \Pr(A|B_j) \Pr(B_j)
\]
where $\{B_i\}$ forms a partition of $\mathcal S$, i.e. $\cup_j B_j = \mathcal S$ and $B_i \cap B_j = \emptyset$.

\section{Weak law of large numbers}
Let $\{X_n : n \geq 1\}$ be a sequence of i.i.d. random variables with
$\mathbb E[|X|] < \infty$. Then, the sample mean converges in probability to the first moment.
Let's prove this result from the ground up.

\subsection{Statement of the Law of Large Numbers} We are given $X_1, X_2, \ldots, X_n$ independent
and identically distributed RVs where $\Pr(X_j \leq x) = F(X)$ denotes 
their common cumulative distribution function. Suppose that $E[X_i]=\mu$
and $\textrm{var}(X_i) = \sigma^2$, then the \emph{weak law states that}
 $\Pr(|\bar{X}_n - \mu| > \delta) \longrightarrow 0$ for any $\delta > 0$,
as $n \uparrow \infty$; i.e.
``with high probability, $\bar{X}_n$ close to $\mu$''. 
On the other hand, the \emph{strong law states that} $\bar{X}_n \longrightarrow \mu$ with probability 1; i.e.
``with a single experiment, if we run it long enough, we \emph{will get} $\mu$''. 

\paragraph{Sample Mean} We are interested in the sample (empirical) mean,
\[
\bar{X}_n = \frac{X_1 + X_2 + \ldots + X_n}{n}.
\]

We expect that the sample mean should be closely related to the theoretical mean $\mu = E[X_j]$. We denote the \emph{variance} of $X_j$ by
\[
\sigma^2 = \textrm{var}(X_j) = E\big[ (X_j - \mu)^2 \big] = \int_{\mathbb R} (x-\mu)^2 d F(x).
\]

\subsection{Markov's Inequality} If $X$ is a non-negative random variable
and $a > 0$, then
\[
\Pr(X \geq a) \leq \frac{\mathbb E[X]}{a}.
\]

\begin{proof}   Given $a > 0$, and non-negative $X$,
  \[
    a \mathbbm 1\{X \geq a\} \leq X
  \]
  is clearly true (consider two cases, when $X \geq a$ and $X < a$).
  Note that $\mathbb E[\cdot]$ is a monotone operator, hence
  \[
    \mathbb E\left[ a \mathbbm 1\{X \geq a\} \right] \leq \mathbb E[X].
  \]
  By linearity of expectation, we may re-express $\mathbb E\left[ a \mathbbm 1\{X \geq a\} \right]$ as
  \[
    a \mathbb E\left[ \mathbbm 1\{X \geq a\} \right] = a \cdot \Pr(X \geq a) + 0 \cdot \Pr(X < a) = a \Pr(X \geq a). 
  \]
  Hence
  \[
    a \Pr(X \geq a) \leq \mathbb E[X].
  \]
  Since $a > 0$, dividing both sides by $a$ yields the final result. \end{proof}

\subsection{Chebyshev's Inequality} Is of the form
\begin{align*}
\Pr(|X-\mu| > \delta) &\leq \frac{\mathbb E \left[ |X - \mu| \right]}{\delta}, \, \, &\delta > 0.
\end{align*}

\begin{proof}   
\ul{Using the definition of variance},
\begin{align*}     
  \sigma^2 &= \textrm{var}(X) = \int_{-\infty}^\infty (t - \mu)^2 f_X(t) dt \\
           &\geq \int_{-\infty}^{\mu - \epsilon} (t - \mu)^2 f_X(t) dt + \int_{\mu + \epsilon}^\infty (t- \mu)^2 f_X(t) dt,   
\end{align*}
  where the last inequality follows from the fact that $f_X(t)$ non-negative, and $(t-\mu)^2$ also non-negative, hence $\left((t-\mu)^2f_X(t)\right)$
non-negative for all $t$. Hence by restricting the range over which we integrate a positive function, we yield a lower-bound. Then, this is
\[
\geq \int_{-\infty}^{\mu - \epsilon} \epsilon^2 f_X(t) dt + \int_{\mu + \epsilon}^\infty \epsilon^2 f_X(t) dt,
\]
since $t \leq \mu - \epsilon \implies \epsilon \leq |t-\mu| \implies \epsilon^2 \leq (t-\mu)^2$. When we re-arrange, and apply the definition of the density function, we see that
\begin{align*}   &= \epsilon^2 \left(\int_{-\infty}^{\mu - \epsilon} f_X(t) dt + \int_{\mu + \epsilon}^{\infty} f_X(t) dt \right) \\
  &= \epsilon^2 \Pr(X \leq \mu - \epsilon \lor X \geq \mu + \epsilon) \\
  &= \epsilon^2 \Pr \left( |x - \mu| \geq \epsilon \right).  \end{align*}
Hence,
\[
  \sigma^2 \geq \epsilon^2 \Pr(|X-\mu| \geq \epsilon),
\]
where dividing through by $\epsilon^2$ yields the desired result. 
\end{proof}

\paragraph{Weak Law of Large Numbers} The WLLN states that $\bar{X}_n \overset{p}{\longrightarrow} \mu$ as $n \uparrow \infty$, i.e. the sample average 
\ul{converges in probability} to the true average. This follows from Chebyschev's Inequality,
\[
\Pr(|\bar{X}_n - \mu| > \delta) \leq \frac{E \big[ (\bar{X}_n - \mu)^2 \big]}{\delta^2} = \frac{\sigma^2}{n\delta^2} \longrightarrow 0
\]
as $n \uparrow \infty$ for all $\delta > 0$. We use the fact that $\bar{X}_n$ converges to $\mu$ also in mean-square, i.e. $E\big[ (\bar{X}_n - \mu)^2 \big] \longrightarrow 0$.

\begin{proof}   By Chebyschev's Inequality,
  \[
    \Pr(|\bar{X}_n - \mu| > \delta) \leq \frac{1}{\delta^2} E\left[ |\bar{X}_n - \mu|^2 \right].
  \]
  Notice that
  \begin{alignat*}{5}     
    \mathbb E\left[ |\bar{X}_n - \mu|^2 \right]
    &= \mathbb E \left[ \left(\frac{x_1 + x_2 + \ldots + x_n}{n} - \mu \right)^2 \right] \\
    &= \mathbb E \left[ \left( \frac{(x_1 - \mu) + \ldots + (x_n - \mu)}{n}\right)^2 \right] \\
    &= \frac{1}{n^2} \mathbb E \left[ \sum_{i=1}^n \sum_{j=1}^n (x_i - \mu) (x_j - \mu) \right] \\
    &= \frac{1}{n^2} \sum_{i} \sum_{j} \mathbb E \left[ (x_i - \mu) (x_j - \mu)\right] \\
    &= \frac{1}{n^2} \sum_i \mathbb E[(x_i-\mu)^2] \hspace{15pt} \textrm{Since } \textrm{cov}(X_i,X_j)=0\\
    &= \frac{1}{n^2} \sum_i \textrm{var}(x_i) \\
    &= \frac{1}{n^2} \textrm{var}(X), \hspace{25pt} \textrm{ where } \textrm{var}(X) < \infty. \end{alignat*}

We used the fact that if two random variables $X,Y$ are independent,
then $\textrm{cov}(X,Y) = 0$.

Hence, as $n\longrightarrow \infty$,
\[
\Pr( |\bar{X}_n - \mu| > \delta) \leq \frac{\textrm{var}(X)}{\delta^2 n} \longrightarrow 0,
\]
for \emph{any} $\delta > 0$. \end{proof}

\paragraph{What if $\pmb{\textrm{var}(X) = + \infty}$?} We can still get to the same result, but we can no longer use Chebyschev's inequality to get there.

\paragraph{What if $\pmb{\{x_j\}}$ dependent?} The result does not hold. There are many counter examples. Suppose all $x_j$ the same. Then the result clearly
does not follow. Hence we need some sort of approximate independence,
i.e. $X_j$ must be sufficiently uncorrelated.

\section{Motivating least squares}
We have a response variable and some relevant explanatory variables.
It would be nice if there were a true function mapping our explanatory variables
to a particular and correct value of the response. We'll assume such a function exists, and
therefore that $y \sim f(X)$. Since we don't
know the functional form of $f$, we instead hope to approximate it via a linear model,
$y \sim X\beta + \epsilon$. 
But how do we determine our fitted-coefficients, $\hat \beta$? It's natural to define
goodness of fit in terms of prediction error; we'll use the $\ell$-2 norm.
\[
  \hat \beta = \argmin_{\beta} \| y - X \beta \|_2^2 = \argmin_\beta \sum_{i=1}^n \left(y_i - \beta_0 - \beta_1 x_{i1} - \ldots - \beta_p x_{ip}\right)^2.
\]

Our least squares estimate falls out immediately.

\[
  \hat \beta = \argmin_{\beta} \|y - X\beta\|^2 = \argmin_\beta (y - X \beta)^T (y - X \beta) = \argmin_\beta -2\beta^T X^T y + \beta^T X^T X \beta \implies \hat \beta = (X^TX)^{-1} X^T y 
\]

This is enough to write a short program to solve for least-squares from scratch. 

\paragraph{Homework exercise: write a short R program to solve for least-squares from scratch} You are allowed to use R's \texttt{solve} command to invert a matrix.\footnote{This is, in practice, a bad idea for reasons relating to numerical stability.}

<<echo=F>>=
linearRegression <- function(y, x, intercept = T) 
    list('coefficients' = NA,  # These are for you
         'covariance'   = NA,  # to fill in...
         'fitted'       = NA,  # after performing some
         'residuals'    = NA,  # computations above...
         'rsq'          = NA)  # Have fun!
@ 

<<echo=T>>=

rSquare <- function(y, yhat, intercept) if (intercept) cor(y, yhat)^2 else sum(yhat^2)/sum(y^2)

sigmaSquare <- function(residuals, p) sum(residuals^2) / (length(residuals) - p)

linearRegression <- function(y, x, intercept = T, alpha = NULL) {
    x <- as.matrix(x)  # Ensure matrix operations available.
    if (!is.null(alpha)) y <- y - alpha
    if (intercept)       x <- cbind(1, x)
    xpx.inv <- solve(t(x) %*% x)
    beta    <- xpx.inv %*% t(x) %*% y
    yhat    <- x %*% beta
    sigma.square   <- sigmaSquare(y-yhat, ncol(x))
    rownames(beta) <- paste0('B', 1:length(beta) - intercept)
    list('coefficients' = beta,
         'covariance'   = xpx.inv * sigma.square,
         'fitted'       = yhat,
         'residuals'    = y - yhat,
         'rsq'          = rSquare(y, yhat, intercept))
}
@ 

But for statistical inference, we'll need to know more about the distributional results of our estimator. More on that later. For now, it's enough to consider the assumption
that $\epsilon \sim \mathcal N(0, I_n)$, i.e. that our error term is normally distributed with
no correlation between observations (i.e. independent) and some constant variance. Why might this
assumption be reasonable? In reality, it's not the most reasonable, but it makes our model mathematically tractable.

\section{Distributional results} If we start off with $Y = X \beta + \epsilon$,
where we assume $\epsilon \sim \mathcal N(0, \sigma^2 I)$, then we have the following.

\paragraph{Distribution of response} Observe that $y$ depends on $\epsilon$, itself
a random variable, hence $y$ is a random variable. It therefore makes sense to consider
its expectation and variance. Using linearity of expectation, and properties of variance,\footnote{In the calculation for variance, we've used the fact that if $Z$ a random variable and $a$
is a constant, then $\textrm{var}(a+Z) = \textrm{var} (Z)$, which can be verified using
the definition of variance.
}

\begin{align*}   \mathbb E[y|X] &= \mathbb E[X\beta + \epsilon | X] = \mathbb E [X \beta | X] + \mathbb E[\epsilon | X] = X \beta. \\
  \textrm{var}(y|X) &= \textrm{var}(X \beta + \epsilon | X) = \textrm{var}(\epsilon|X) = \sigma^2 I. \end{align*}


\paragraph{First and second moment of coefficients} We have a formula for $\hat \beta = (X^TX)^{-1} X^T y$.
Since $y$ is random, and since $\hat \beta$ is a function of $y$, then this implies that $\hat \beta$ also a random variable.\footnote{We caveat that the population parameter,  
  $\beta$, is a constant and \emph{not} a random variable.} We start by considering the
expectation of our estimator.

\begin{align*}   \mathbb E [\hat \beta | X] &= \mathbb E\left[(X^TX)^{-1} X^T y | X \right] = (X^TX)^{-1} X^T \underbrace{\mathbb E[y|X]}_{X \beta} = \beta, \\ \end{align*}

Note that we are assuming our data are fixed (i.e. \emph{not} random); we can remove this
assumption by first conditioning on a given dataset, then taking the expectation over all data,
i.e. $\mathbb E[X] = \mathbb E[\mathbb E[X|Y]]$. In this case, our estimates 
$\hat \beta$ still unbiased, since $\mathbb E[\hat \beta]] = \mathbb E[\mathbb E[\hat \beta|X]] = \mathbb E [ \beta] = \beta$. %% We remark that our estimator is unbiased whenever
%% $\mathbb E[\epsilon|X] = 0$ and $\textrm{var}(\epsilon|X) = \sigma^2 I$.

This leads us to question, what about the variance of our estimator?\footnote{In the calculation for variance, we've used the fact that if $Z$ a random variable and $a$
is a constant, then $\textrm{var}(aZ) = a^2 \textrm{var} (Z)$, which can be verified using
the definition of variance.
}

\begin{align*}   \textrm{var}(\hat \beta | X) &= \textrm{var}\left((X^TX)^{-1}X^T y |X\right) = (X^TX)^{-1} X^T \underbrace{\textrm{var}(y)}_{\sigma^2 I} X (X^TX)^{-T}  = \sigma^2 (X^TX)^{-1}. \end{align*}


Larger values of $X^TX$ lead to smaller variance. %% We remark that our estimator is unbiased
%% when errors are uncorrelated (i.e. $\mathbb E[\epsilon|X] = 0$) and that our population variance
%% a scalar multiple of an identity matrix, then we have that
%% $\hat \sigma^2 = \frac{\texttt{RSS}}{n-p'}$ is unbiased. Here, $n-p'$ denotes the effective degrees of freedom, and $p' = p+\mathbbm 1\{\textrm{intercept used}\}$. Of course, $p$ is the number of features in our design matrix $X$.

\paragraph{Distribution of $\pmb{\hat\beta}$}
Suppose for a moment that $X$ is full-rank; if not, remove features until it is full-rank. Then, 
notice that in specifying $y=X \beta + \epsilon$ and that
$\hat \beta = (X^TX)^{-1} X^T y$, the implication is that
\[
  \hat\beta = (X^TX)^{-1} X^T y = (X^TX)^{-1} X^T (X \beta + \epsilon) = \beta + (X^TX)^{-1} X^T \epsilon.
\]

Here, we are suggesting that $\hat\beta$ is a linear transformation of our error $\epsilon$,
i.e. $\hat\beta$ is a linear transformation of a normally distributed random variable. We know
from probability that linear transformations of normally distributed random variables yield 
yet another normal random variable.\footnote{This can be shown using distribution functions, directly.} Hence $\hat\beta$ is normally distributed, 
and its first two moments alongside
its distribution function fully specify the random variable. I.e.

\[
  \hat \beta \sim \mathcal N(\beta, \sigma^2 (X^TX)^{-1})
\]

How do we estimate $\hat \beta$ in practice? We need to solve for $\hat \beta = (X^TX)^{-1} X^T y$,
but yet we can't invert $(X^TX)$ directly: it's costly
and prone to numerical inaccuracies. Realize instead that we just showed that $\hat \beta \sim \mathcal N$, and recall that there exists a method called Maximum Likelihood Estimation which finds the parameters to make the observed data most likely under the specified distribution. 
To that end, we turn to MLE, which will get us an estimate of 
$\hat \beta$ to within machine precision.

\section{Maximum likelihood estimation}
We now derive some theory behind a common type of estimator.

\paragraph{Set Up} Suppose $X$ a random variable with density $f(x|\theta)$ 
depending on parameter $\theta$, which is unknown; $\theta \in \Theta \subseteq \Bbb R$. We denote the theoretical true value by $\theta^*$ (itself
unknown). We draw a sample $X_1, X_2, \ldots, X_n$ i.i.d. from
$f(x|\theta^*)$.

\paragraph{Problem} Estimate $\theta^*$ from the sample, i.e.
find $\hat \theta_n(x_1, x_2, \ldots, x_n)$ such that $\hat \theta_n \approx \theta^*$ for $n$ large enough.

\paragraph{Example} $f(x|\mu)$ is a density with parameter $\mu$, its mean.
Suppose we let $\hat \theta_n = \bar X_n$. 

\paragraph{What do we want from our estimator?}
We know that (by WLLN) 
$\hat \theta_n \overset{P}{\longrightarrow} \mu^*,$ the true mean. What do we want
from our estimator $\hat \theta_n$?

(1) Consistency: $\hat \theta_n \overset{P}{\longrightarrow} \theta^*$, i.e.
we want our estimator to go to the right value.

(2) Efficiency: We also want our estimator to get to the right value as fast as possible.
\emph{If} we have the asymptotic result that
  $\sqrt n (\hat \theta_n - \theta^*) \overset{ \mathcal D}{\longrightarrow} \mathcal N (0, \sigma^2)$,
\emph{then} $\sigma^2$ is the smallest possible among consistent estimators.

\subsubsection{Derivation}
We draw $X_1, X_2, \ldots, X_n$ i.i.d. sample from $f(x|\theta^*)$.
We are given the data, we do not know $\theta^*$.
We assume Regularity Conditions such as differentiability of 
$f(x|\theta)$ with respect to $\theta$.
Examine the Likelihood Function,
\[
L_n(\theta) = f(x_1|\theta) f(x_2|\theta) \ldots f(x_n|\theta),
\]
where $\prod_{i=1}^nf(x_i|\theta)$ denotes the joint density of 
$x_i$ drawn i.i.d. from $f(x|\theta)$. Let 
\[
\hat \theta_n = \argmax_{\theta \in \Theta} L_n(\theta).
\]

Notice that $\log$ is a monotone transform, and $f(x_j)$ non-negative,
hence $\prod_{k=1}^n f(x_k)$ non-negative. Then,

\[
\ell_n(\theta) = \frac{1}{n} \log L_n (\theta) = \frac{1}{n} \sum_{j=1}^n \log f(x_j|\theta).
\]

By WLLN,
\[
\ell_n(\theta) \overset{P}{\longrightarrow} \int_{-\infty}^\infty \log f(x|\theta) f(x|\theta^*) \D x,
\]
i.e. as $n \uparrow \infty$, $\Pr(|\ell_n (\theta) - \ell(\theta)| > \delta) \longrightarrow 0$ for all $\delta > 0$ and for each $\theta \in \Theta$.

In the section which follows, $f'$ always denotes derivative with 
respect to $\theta$.

\begin{alignat*}{5}
&\ell(\theta) &&= \int \log f(x|\theta) \cdot f(x|\theta^*) \D x \implies \ell '(\theta) = \int_{-\infty}^\infty \frac{f'(x|\theta)}{f(x|\theta)} f(x|\theta^*) \D x \hspace{25pt} &&\textrm{by chain rule} \\
\implies &\ell '(\theta^*) &&= \int_{-\infty}^\infty f'(x|\theta^*) \D x = \frac{\D}{\D \theta} \left( \int_{-\infty}^\infty f(x|\theta) \D x \right) \bigg|_{\theta = \theta^*} = 0. \end{alignat*}

Since $\ell'(\theta^*) = 0$, at least we know that $\theta^*$ is a 
critical point of $\ell(\theta)$.
We know that $\ell_n(\theta) \overset{P}{\longrightarrow} \ell(\theta)$ by WLLN.
Further, \href{http://www.amazon.com/Course-Sample-Chapman-Statistical-Science/dp/0412043718}{Ferguson (1996)}, this holds uniformly in $\theta$ for
\[
| \theta - \theta^*| \leq a.
\]
Hence for all $\delta > 0$,
\[
\Pr\left( \max_{|\theta - \theta^*| \leq a} \bigg|\ell_n(\theta) - \ell(\theta)\bigg| > \delta \right) \overset{n \uparrow \infty}{\longrightarrow} 0.
\]

Does this imply that $\hat \theta_n = \argmax_{|\theta - \theta^*| \leq a} \ell_n(\theta) \overset{P}{\longrightarrow} \theta^*$?
Yes! \href{http://projecteuclid.org/euclid.aoms/1177729952}{Wald (1948)}.

\subsubsection{Applying MLE in R}
Let's see how we can apply what we've learned in our favorite programming language, R!

<<>>=

### For use of the `%>%` operator.
require(magrittr)

### Simply a normal density. Also built-in to R: see dnorm.
MyDensity <- function(x, mu = 0, ss = 1) 
    1/sqrt(2*pi*ss) * exp(-(mu-x)^2/(2*ss))

### Returns minus the log-likelihood of a normally distributed RV.
LL <- function(beta, response, data)
    -1 * MyDensity(response - (data %>% as.matrix) %*% beta) %>% log %>% sum

@ 

\paragraph{Simulating Data}
Now that we've defined our functions, let's simulate some data.

<<>>=

N <- 10
P <- 2
X <- matrix(rnorm(P*N), nrow = N)
Y <- runif(N)

@ 

\paragraph{Running a regression}
Great, now we run a regression and compare the results.

<<>>=

m <- optim(par = c('beta' = rep(0, ncol(X))), fn = LL, response = Y, data = X)

@ 

\paragraph{Determining Correctness}
We verify that our regression well set-up.

<<>>=
m$par - lm(Y ~ 0 + X)$coef < 1e-3
@ 


\section{Hypothesis testing} Asdf

\subsection{Sign Tests}
Suppose 10 patients were given treatment for a disease, and their survival times reported in weeks.
One patient outlived the study. The survival times were

<<>>=
surv <- c(49, 58, 75, 110, 112, 132, 151, 276, 281, 362)
@ 

Where we caveat that the last observation represents the censored patient who was still
alive at the end of the 362 week study. We ask ourselves: was the median survival time less than or greater than 200 weeks? The null hypothesis is that median survival is 200 weeks.

We create a new series of data by comparing each existing observation with 200;
we output a \texttt{-} or \texttt{+} correspondingly. We now reframe our question:
if the median survival time was 200 weeks, how many $\pm$'s would you expect to see?
Under the null, the number of \texttt{-}'s will have a binomial distribution with probability of success $p=1/2$. So, the probability of observing data at least as extreme as we did is given by
\[
\Pr(\# \texttt{-} = k) = \binom{n}{k} \frac{1}{2^k} \frac{1}{2^{n-k}} 
\]
In our data, we observe 7 out of 10 observations less than 200. Under the null,
the probability of observing an outcome at least as extreme as we did is given by
\[
  \sum_{\substack{k=1 \\ k \neq 4,5,6}}^{10} \Pr(\# \texttt{-} = k) = \frac{1}{2}^{10} \sum_{k \in \{0,1,2,3,7,8,9,10\}} \binom{10}{k} \approx 0.3438.
\]


To see this in R,

<<>>=

### Remember, we're interested in a two-tail test.
nck <- choose(10, c(0:3, 7:10))
1/2^(10) * sum(nck)

@ 

We can do this a bit more simply, using R's built-in functions.

<<>>=
binom.test(x = 7, n = 10, p = 0.5, alternative = 'two.sided')
@ 

This is well and good, but what if we don't know the distribution of our test-statistic?
E.g. above we knew that if the median survival rate was 200 weeks, then it would be ``like a coin-toss'' whether an observation met the threshold. However, this took insight. When we don't know
the distribution of the test-statistic, we need to have a non-parametric test.

\subsection{Permutation Testing}
Let's examine whether there's a relationship between \texttt{distance} traveled
and \texttt{speed} in our \texttt{Cars} dataset. How can we go about this?
Well, if there was no relationship between the two variables, then permuting one
and not the other should leave the relationship unchanged. To get a measure of
how likely it is to observe the relationship we do, conditional on there being
no true relationship, we can apply a \emph{permutation test}.

\begin{algorithm}
  Compute a test statistic between $k$ groups, e.g. a difference of means \\
  Randomly shuffle the data assigned to each group \\
  Measure the test-statistic on the shuffled data \\
  Repeat steps 2 and 3 many times \\
  Compare the test-statistic from step 1 to those obtained in steps  2-3
\end{algorithm}
<<cache=T>>=

tru.model <- lm(speed ~ dist, data = cars)

stats <- replicate(1e4, expr = {
    prmtd <- sample(1:nrow(cars), replace = F)
    lm(speed ~ I(dist[prmtd]), data = cars)$coef
})
stats <- stats[2, ]  # Fix attention to slope-coefficient only.

### Estimated p-value of observing data at least as extreme.
mean(abs(stats) >= abs(tru.model$coef['dist']))

@ 

\subsection{Bootstrapping confidence intervals}

\begin{itemize}   \item Our sample gives a sense of the population.
  \item In reality, we only have one sample and we don't get to see the entire population.
  \item As a best guess for the population, we can use the sample we actually observed. \end{itemize}

<<cache=T>>=
require(MASS)  # For dataset: Boston.
N <- nrow(Boston)

### Run one model.
model <- lm(crim ~ tax + medv, data = Boston)

### Run a series of models, each time bootstrapping a coefficient.
coefs <- replicate(1e4, expr = lm(crim ~ tax + medv, 
                                  data = Boston[sample(1:N, replace = T), ])$coef['tax'])
@

With our statistics in hand, we're ready to bootstrap an estimated confidence interval for one of our coefficients.


<<echo=F,fig.height=4.5,cache=T>>=

my <- quantile(coefs, probs = c(0.025, 0.975))
ci <- confint(model, parm = 'tax')

hist(coefs, prob = T, main = 'Histogram of coefficients', xlab = 'Estimated coefficient')
abline(v = ci[1], lty = 'dashed', col = 'purple')
abline(v = ci[2], lty = 'dashed', col = 'purple')
abline(v = my[1], lty = 'dashed', col = 'orange')
abline(v = my[2], lty = 'dashed', col = 'orange')
legend(x = 0.0325, y = 100, col = c('purple', 'orange'), lty = 'dashed', 
       legend = c('R', 'Bootstrap'))


@ 

\section{Statistical Inference}
Lastly, we investigate the cornerstone of statistical inference: the hypothesis test. \subsection{Hypothesis testing} \subsection{Confidence intervals}

\section{Homework/Practice}
\section{Problem \#0} 
%% Write an R function which samples the elements from the following  table with
%% their corresponding probabilities.
%% \begin{table}
%%   & A & B & C \\
%%   \textrm{Probability} & 0.4 & 0.3 & 0.1
%% \end{table}

<<cache=T>>=

MySample <- function(tbl, n) {
    replicate(n, expr = {
        cums <- cumsum(tbl)
        coin <- runif(1)
        if (all(coin < cums)) return(tbl[1])
        if (all(coin > cums)) return (tbl[length(tbl)])
        names(cums)[which.min(coin > cums)]
    })
}

MySample(tbl = c("A" = 0.45, "B" = 0.25, "C" = 0.35), n = 10)

@ 

 \section*{Problem \#1}  \paragraph{Question} A target is made of 3 concentric circles of radii $1/\sqrt{3}$, 1, and $\sqrt{3}$ feet. Shots within the inner circle are given 4 points, shots within the next ring are given 3 points, and shots within the third ring are given 2 points. Shouts outside the target are given 0 points.  Let $X$ denote the distance of the hit from the center (in feet), and let the p.d.f. of $X$ be  \[ f(x) = \frac{2}{\pi (1 + x^2)} \boldsymbol{1}\{x > 0\} \]  What is the expected value of the score of a single shot?  \paragraph{Solution} Using the definition of the expectation of a discrete-valued random variable, we have that  \begin{align*} \mathbb E[\text{single shot}] &= \sum_{x \in X} x \cdot \Pr(X = x) \\  &= 0 \cdot \Pr(X > \sqrt{3}) + 2 \cdot \Pr(1 < X < \sqrt{3}) + 3 \cdot \Pr(\frac{1}{\sqrt{2}} < X < 1) + 4 \cdot \Pr(X < \frac{1}{\sqrt{3}}) \end{align*}  The solution is simple once we solve for the c.d.f. of our random variable $X$. By definition,  \[ F(x) = \int_0^x f(x) dx = \int_0^x \frac{2}{\pi(1 + x^2)} dx \]  Recall that when $\sin \theta$ and $\cos \theta$ are defined using a unit circle, the $(x,y)$ point which lay on the unit circle is given by $(\cos \theta, \sin \theta)$, where $\theta$ denotes the angle from the origin.  Therefore, by the Pythagorean theorem, we have the identity that $\sin^2 \theta + \cos^2 \theta = 1$, since we can describe any point on the unit circle by a (possible degenerate) right triangle. We may then derive that  \begin{align*} \sin^2 \theta + \cos^2 \theta &= 1 \\ \frac{\sin^2 \theta}{\cos^2 \theta} + \frac{\cos^2\theta}{\cos^2 \theta} &= \frac{1}{\cos^2 \theta} \\ \tan^2 \theta + 1 &= \sec^2 \theta \end{align*}  This informs how we may use $u$-substitution to solve for our c.d.f. Let $x = \tan \theta$. Then,  \begin{align*} \frac{d}{d \theta} \tan \theta &= \frac{d}{d \theta} \frac{\sin \theta}{\cos \theta} \\  &= \frac{d}{d \theta} \sin \theta \cos^{-1} \theta \\  &= \frac{\cos \theta}{\cos \theta} + \frac{-\sin \theta}{\cos^2 \theta} \cdot (-\sin \theta) \\  &= \frac{\cos^2 \theta + \sin^2 \theta}{\cos^2 \theta} \\ &= \frac{1}{\cos^2 \theta} \\ &= \sec^2 \theta \end{align*}  If $x = \tan \theta$, then $\theta = \arctan x$, and further  \[ \frac{dx}{d\theta} = \sec^2 \theta \implies dx = \sec^2 \theta d \theta. \]  This is great because we can use $u$-substitution to reduce our integral to something very simple:  \[ \int \frac{1}{1 + x^2}dx = \int \frac{1}{\sec^2 \theta} \sec^2 \theta d\theta = \int d \theta = \theta + C \]  Therefore,  \begin{align*} \frac{2}{\pi} \int_0^x  \frac{1}{1+x^2}dx &= \frac{2}{\pi} \arctan x \bigg |_0^x = \frac{2}{\pi} \arctan x \end{align*}  Note that $\arctan 0 = 0$ and  $\underset{x \to \infty}{\lim} \arctan x \to \pi/2$, so that  $F(0) = 0$ and $\underset{x \to \infty}{\lim} F(x) \to 1$, which is how we expect our c.d.f. to behave for this random variable.  Now,  \begin{align*} \mathbb{E}[\text{single shot}] &= \sum_{x \in X} x \cdot \Pr(X = x) \\  &= 0 \cdot \Pr(X > \sqrt{3}) + 2 \cdot \Pr(1 < X < \sqrt{3}) + 3 \cdot \Pr(\frac{1}{\sqrt{2}} < X < 1) + 4 \cdot \Pr(X < \frac{1}{\sqrt{3}}) \\ &= 2 \bigg( F(\sqrt{3}) - F(1) \bigg) + 3 \bigg(F(1) - F(1/\sqrt{3}) \bigg) + 4 F(1/\sqrt{3}) \\ &\approx 3.403  \end{align*}  \section*{Problem \#2}   \paragraph{Question} Assume that the random variable $X$ has the exponential distribution  \[ f(x|\theta) = \theta e^{-\theta x} \boldsymbol{1}\{x > 0, \theta > 0\} \]  where $\theta$ is the parameter of the distribution. Use the method of maximum likelihood to estimate $\theta$ is 5 observations of $X$ are $x_1 = 0.9$, $x_2 = 1.7$, $x_3 = 0.4$, $x_4 = 0.3$, and $x_5 = 2.4$ are generated independent and identically distributed.  \paragraph{Answer} We are told that  \[ x_1, \ldots, x_n \overset{i.i.d.}{\sim} \textrm{exp} (\theta) \]  By independence, we have that  \[ f(x_1, \ldots, x_n | \theta_1, \ldots, \theta_n) = \prod_{i=1}^n f(x_i|\theta_i) \]  and by identically distributed, we have that  \[ \prod_{i=1}^n f(x_i) = \theta e^{-\theta x_i} \]  Therefore,  \[ f(x_1, \ldots, x_n|\theta) = \prod_{i=1}^n \theta e^{- \theta x_i} = \theta^n e^{-\theta \sum_i x_i} \]  Our likelihood function is given by  \[ L(\theta|x_1, \ldots, x_n) = \theta^n e^{-\theta \sum_i x_i}. \]  Maximizing a monotonic increasing transformation of the likelihood is the same as maximizing the likelihood, so to make life easier we take the log-likelihood:  \[ \lg L(\theta|x_1, \ldots, x_n) = n \lg \theta - \theta \sum_i x_i \]  Maximizing this by taking the derivative and setting it to zero shows that   \begin{align*} \frac{d}{d \theta} \lg L(\theta |x_1, \ldots, x_n) = \frac{n}{\theta} - \sum_i x_i &= 0 \\ \frac{n}{\theta} &= \sum_i x_i \\ \implies \hat{\theta}_{\text{mle}} &= \frac{n}{\sum_i x_i} \end{align*}  In other words, our maximum likelihood estimator for our parameter $\theta$ is simply the reciprocal of the sample mean. Note that we can convince ourselves that we have found a true maximum by taking the second derivative of our likelihood function and checking that it is concave, yielding  $\frac{d}{d \theta} \frac{n}{\theta} - \sum_i x_i = \frac{-n}{\theta^2}$, which is an upside down parabola and therefore has a single maximum point.  If $x = (0.9, 1.7, 0.4, 0.3, 2.4)$, then $\sum_i x_i = 5.7$, and $\bar{x} = 57/50 = 1.14$. Therefore, we see that  \[ \hat{\theta}_{\text{mle}} = \frac{1}{\bar{x}} = \frac{1}{1.14} \approx 0.877. \] 

\bibliographystyle{plain}
\begin{thebibliography}{10}
\bibitem{clain: ec2120}
G. Chamberlain.
\newblock {\em Econometrics 2120}.
\newblock Harvard University, Winter 2017.

\bibitem{papa: 308}
G. Papanicalou.
\newblock {\em CME 308: Stochastic Processes}.
\newblock Stanford University, Spring 2016.

\bibitem{ferguson: lst}
Thomas S. Ferguson
\newblock {\em A course in large sample theory}.
\newblock University of California, Los Angeles.

\bibitem{Wald: 48}
Abraham Wald.
\newblock {\em Note on the consistency of the maximum likelihood estimate}.
\newblock Annals of Mathematical Statistics, 1949.

\bibitem{islr: intro stat learning}
James et. al.
\newblock {\em An introduction to statistical learning}.
\newblock Springer 2017.


\bibitem{bstr: bstrapefron}
Bradley Efron.
\newblock {\em Bootstrap methods: another look at the jackknife}.
\newblock Annals of Statistics, 1979.

\bibitem{fin: finetti}
Bruno de Finetti.
\newblock {\em Theory of Probability}.
\newblock 1970, Wiley.


\bibitem{taylor: jtaylor}
Johnathan Taylor.
\newblock {\em Data Science 101}.
\newblock Stanford University, 2016.

\end{thebibliography}

% Bstrapping prediction error?
% Handling 0-1 response variables
% A simple logistic regression?
% Handling categorical variables?

% Stats 305 notes section 9.6 (distributional results of OLS estimators).
% Data Science 101 notes.

\end{document}
