\documentclass[12pt]{article}  \usepackage{algorithm2e} \usepackage{amsmath} \usepackage{amsthm} \usepackage{amsfonts} \usepackage{bbm} \usepackage{color,soul} \usepackage{framed} \usepackage[margin=0.5in]{geometry} \usepackage{hyperref} \usepackage{mathtools} \usepackage[dvipsnames]{xcolor} 
\usepackage{tikz} \newtheorem{theorem}{Theorem}[section] \newtheorem{lemma}[theorem]{Lemma} \newtheorem{proposition}[theorem]{Proposition} \newtheorem{corollary}[theorem]{Corollary}  \DeclarePairedDelimiter{\ceil}{\lceil}{\rceil} \DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor} \DeclareMathOperator*{\argmin}{arg\,min} \DeclareMathOperator*{\argmax}{arg\,max} \newcommand{\D}{\mathrm{d}} \SetKwInput{KwInput}{Input} \SetKwInput{KwOutput}{Output}  \begin{document}

\title{\textbf{Tenants of Probability}}
\author{Andreas Santucci}
\date{August 2017}
\maketitle

\section{Introduction to Probability}
Basic concepts of probability are more easily explained in a finite state space.
There is a finite set
\[
\mathcal S = \{s_1, \ldots, s_M\}
\]
whose elements are are said to be \emph{elementary events}. 
A \emph{probability measure} assigns a real number $P(s)$ to each state $s \in S$.
One and only one of the states will occur, and that's reflected in the probabilities being
non-negative and summing to one.

\begin{align*}   \Pr(s_j) \geq 0, \hspace{25pt} \sum_{j=1}^M \Pr(s_j) = 1. \end{align*}

An \emph{event} $A$ is a subset of state-space $S$, and it is said to occur if it contains the state that occurs. I.e.
\[
\Pr(A) = \sum_{s \in A} \Pr(s).
\]

We take as axiom the following rule of conditional probability, 
$\Pr(A,B) = \Pr(A) \cdot \Pr(B | A)$.
A \emph{random variable} is a mapping from the state-space to the real-numbers. Its expectation is 
formed by a weighted average,
\[
\mathbb E[Y] = \sum_{j=1}^{M} Y(s_j) \Pr(s_j).
\]

We have the \emph{law of total probability},
\[
\Pr(A) = \sum_{j} \Pr(A|B_j) \Pr(B_j)
\]
where $\{B_i\}$ forms a partition of $\mathcal S$, i.e. $\cup_j B_j = \mathcal S$ and $B_i \cap B_j = \emptyset$.

\section{Weak law of large numbers}
Let $\{X_n : n \geq 1\}$ be a sequence of i.i.d. random variables with
$\mathbb E[|X|] < \infty$. Then, the sample mean converges in probability to the first moment.
Let's prove this result from the ground up.

\subsection{Statement of the Theorem} We are given $X_1, X_2, \ldots, X_n$ independent
and identically distributed RVs where $\Pr(X_j \leq x) = F(X)$ denotes 
their common cumulative distribution function. Suppose that $E[X_i]=\mu$
and $\textrm{var}(X_i) = \sigma^2$, then the \emph{weak law states that}
 $\Pr(|\bar{X}_n - \mu| > \delta) \longrightarrow 0$ for any $\delta > 0$,
as $n \uparrow \infty$; i.e.
``with high probability, $\bar{X}_n$ close to $\mu$''. 
On the other hand, the \emph{strong law states that} $\bar{X}_n \longrightarrow \mu$ with probability 1; i.e.
``with a single experiment, if we run it long enough, we \emph{will get} $\mu$''. 

\paragraph{Sample Mean} We are interested in the sample (empirical) mean,
\[
\bar{X}_n = \frac{X_1 + X_2 + \ldots + X_n}{n}.
\]

We expect that the sample mean should be closely related to the theoretical mean $\mu = E[X_j]$. We denote the \emph{variance} of $X_j$ by
\[
\sigma^2 = \textrm{var}(X_j) = E\big[ (X_j - \mu)^2 \big] = \int_{\mathbb R} (x-\mu)^2 d F(x).
\]

\subsection{Markov's Inequality} If $X$ is a non-negative random variable
and $a > 0$, then
\[
\Pr(X \geq a) \leq \frac{\mathbb E[X]}{a}.
\]

\begin{proof}   Given $a > 0$, and non-negative $X$,
  \[
    a \mathbbm 1\{X \geq a\} \leq X
  \]
  is clearly true (consider two cases, when $X \geq a$ and $X < a$).
  Note that $\mathbb E[\cdot]$ is a monotone operator, hence
  \[
    \mathbb E\left[ a \mathbbm 1\{X \geq a\} \right] \leq \mathbb E[X].
  \]
  By linearity of expectation, we may re-express $\mathbb E\left[ a \mathbbm 1\{X \geq a\} \right]$ as
  \[
    a \mathbb E\left[ \mathbbm 1\{X \geq a\} \right] = a \cdot \Pr(X \geq a) + 0 \cdot \Pr(X < a) = a \Pr(X \geq a). 
  \]
  Hence
  \[
    a \Pr(X \geq a) \leq \mathbb E[X].
  \]
  Since $a > 0$, dividing both sides by $a$ yields the final result. \end{proof}

\subsection{Chebyshev's Inequality} Is of the form
\begin{align*}
\Pr(|X-\mu| > \delta) &\leq \frac{\mathbb E \left[ |X - \mu| \right]}{\delta}, \, \, &\delta > 0.
\end{align*}

\begin{proof}   
\ul{Using the definition of variance},
\begin{align*}     
  \sigma^2 &= \textrm{var}(X) = \int_{-\infty}^\infty (t - \mu)^2 f_X(t) dt \\
           &\geq \int_{-\infty}^{\mu - \epsilon} (t - \mu)^2 f_X(t) dt + \int_{\mu + \epsilon}^\infty (t- \mu)^2 f_X(t) dt,   
\end{align*}
  where the last inequality follows from the fact that $f_X(t)$ non-negative, and $(t-\mu)^2$ also non-negative, hence $\left((t-\mu)^2f_X(t)\right)$
non-negative for all $t$. Hence by restricting the range over which we integrate a positive function, we yield a lower-bound. Then, this is
\[
\geq \int_{-\infty}^{\mu - \epsilon} \epsilon^2 f_X(t) dt + \int_{\mu + \epsilon}^\infty \epsilon^2 f_X(t) dt,
\]
since $t \leq \mu - \epsilon \implies \epsilon \leq |t-\mu| \implies \epsilon^2 \leq (t-\mu)^2$. When we re-arrange, and apply the definition of the density function, we see that
\begin{align*}   &= \epsilon^2 \left(\int_{-\infty}^{\mu - \epsilon} f_X(t) dt + \int_{\mu + \epsilon}^{\infty} f_X(t) dt \right) \\
  &= \epsilon^2 \Pr(X \leq \mu - \epsilon \lor X \geq \mu + \epsilon) \\
  &= \epsilon^2 \Pr \left( |x - \mu| \geq \epsilon \right).  \end{align*}
Hence,
\[
  \sigma^2 \geq \epsilon^2 \Pr(|X-\mu| \geq \epsilon),
\]
where dividing through by $\epsilon^2$ yields the desired result. 
\end{proof}

\paragraph{Weak Law of Large Numbers} The WLLN states that $\bar{X}_n \overset{p}{\longrightarrow} \mu$ as $n \uparrow \infty$, i.e. the sample average 
\ul{converges in probability} to the true average. This follows from Chebyschev's Inequality,
\[
\Pr(|\bar{X}_n - \mu| > \delta) \leq \frac{E \big[ (\bar{X}_n - \mu)^2 \big]}{\delta^2} = \frac{\sigma^2}{n\delta^2} \longrightarrow 0
\]
as $n \uparrow \infty$ for all $\delta > 0$. We use the fact that $\bar{X}_n$ converges to $\mu$ also in mean-square, i.e. $E\big[ (\bar{X}_n - \mu)^2 \big] \longrightarrow 0$.

\begin{proof}   By Chebyschev's Inequality,
  \[
    \Pr(|\bar{X}_n - \mu| > \delta) \leq \frac{1}{\delta^2} E\left[ |\bar{X}_n - \mu|^2 \right].
  \]
  Notice that
  \begin{alignat*}{5}     
    E\left[ |\bar{X}_n - \mu|^2 \right]
    &= E \left[ \left(\frac{x_1 + x_2 + \ldots + x_n}{n} - \mu \right)^2 \right] \\
    &= E \left[ \left( \frac{(x_1 - \mu) + \ldots + (x_n - \mu)}{n}\right)^2 \right] \\
    &= \frac{1}{n^2} E \left[ \sum_{i=1}^n \sum_{j=1}^n (x_i - \mu) (x_j - \mu) \right] \\
    &= \frac{1}{n^2} \sum_{i} \sum_{j} E \left[ (x_i - \mu) (x_j - \mu)\right] \\
    &= \frac{1}{n^2} \sum_i E[(x_i-\mu)^2] \hspace{15pt} \textrm{Since } \textrm{cov}(X_i,X_j)=0\\
    &= \frac{1}{n^2} \sum_i \textrm{var}(x_i) \\
    &= \frac{1}{n^2} \textrm{var}(X), \hspace{25pt} \textrm{ where } \textrm{var}(X) < \infty. \end{alignat*}

We used the fact that if two random variables $X,Y$ are independent,
then $\textrm{cov}(X,Y) = 0$.

Hence, as $n\longrightarrow \infty$,
\[
\Pr( |\bar{X}_n - \mu| > \delta) \leq \frac{\textrm{var}(X)}{\delta^2 n} \longrightarrow 0,
\]
for \emph{any} $\delta > 0$. \end{proof}

\paragraph{What if $\pmb{\textrm{var}(X) = + \infty}$?} We can still get to the same result, but we can no longer use Chebyschev's inequality to get there.

\paragraph{What if $\pmb{\{x_j\}}$ dependent?} The result does not hold. There are many counter examples. Suppose all $x_j$ the same. Then the result clearly
does not follow. Hence we need some sort of approximate independence,
i.e. $X_j$ must be sufficiently uncorrelated.

\section{Motivating least squares}
We have a response variable and some relevant explanatory variables.
It would be nice if there were a true function mapping our explanatory variables
to a particular and correct value of the response. We'll assume such a function exists, and
therefore that $y \sim f(X)$. Since we don't
know the functional form of $f$, we instead hope to approximate it via a linear model,
$y \sim X\beta + \epsilon$. 
But how do we determine our fitted-coefficients, $\hat \beta$? It's natural to define
goodness of fit in terms of prediction error; we'll use the $\ell$-2 norm.
\[
  \hat \beta = \argmin_{\beta} \| y - X \beta \|_2^2 = \argmin_\beta \sum_{i=1}^n \left(y_i - \beta_0 - \beta_1 x_{i1} - \ldots - \beta_p x_{ip}\right)^2.
\]

Our least squares estimate falls out immediately.

\[
  \hat \beta = \argmin_{\beta} \|y - X\beta\|^2 = \argmin_\beta (y - X \beta)^T (y - X \beta) = \argmin_\beta -2\beta^T X^T y + \beta^T X^T X \beta \implies \hat \beta = (X^TX)^{-1} X^T y 
\]

This is enough to write a short program to solve for least-squares from scratch. 

<<>>=

rSquare <- function(y, yhat, intercept) if (intercept) cor(y, yhat)^2 else sum(yhat^2)/sum(y^2)

sigmaSquare <- function(residuals, p) sum(residuals^2) / (length(residuals) - p)

linearRegression <- function(y, x, intercept = T, alpha = NULL) {
    x <- as.matrix(x)  # Ensure matrix operations available.
    if (!is.null(alpha)) y <- y - alpha
    if (intercept)       x <- cbind(1, x)
    xpx.inv <- solve(t(x) %*% x)
    beta    <- xpx.inv %*% t(x) %*% y
    yhat    <- x %*% beta
    sigma.square   <- sigmaSquare(y-yhat, ncol(x))
    rownames(beta) <- paste0('B', 1:length(beta) - intercept)
    list('coefficients' = beta,
         'covariance'   = xpx.inv * sigma.square,
         'fitted'       = yhat,
         'residuals'    = y - yhat,
         'rsq'          = rSquare(y, yhat, intercept))
}
@ 

But for statistical inference, we'll need to know more about the distributional results of our estimator. More on that later. For now, it's enough to consider the assumption
that $\epsilon \sim \mathcal N(0, I_n)$, i.e. that our error term is normally distributed with
no correlation between observations (i.e. independent) and some constant variance. Why might this
assumption be reasonable? In reality, it's not the most reasonable, but it makes our model mathematically tractable.

\section{Estimation}
We next show derive some theory behind a common type of estimator.

\subsection{Maximum likelihood estimation}

\paragraph{Set Up} Suppose $X$ a random variable with density $f(x|\theta)$ 
depending on parameter $\theta$, which is unknown; $\theta \in \Theta \subseteq \Bbb R$. We denote the theoretical true value by $\theta^*$ (itself
unknown). We draw a sample $X_1, X_2, \ldots, X_n$ i.i.d. from
$f(x|\theta^*)$.

\paragraph{Problem} Estimate $\theta^*$ from the sample, i.e.
find $\hat \theta_n(x_1, x_2, \ldots, x_n)$ such that $\hat \theta_n \approx \theta^*$ for $n$ large enough.

\paragraph{Example} $f(x|\mu)$ is a density with parameter $\mu$, its mean.
Suppose we let $\hat \theta_n = \bar X_n$. 

\paragraph{What do we want from our estimator?}
We know that (by WLLN) 
$\hat \theta_n \overset{P}{\longrightarrow} \mu^*,$ the true mean. What do we want
from our estimator $\hat \theta_n$?

(1) Consistency: $\hat \theta_n \overset{P}{\longrightarrow} \theta^*$, i.e.
we want our estimator to go to the right value.

(2) Efficiency: We also want our estimator to get to the right value as fast as possible.
\emph{If} we have the asymptotic result that
  $\sqrt n (\hat \theta_n - \theta^*) \overset{ \mathcal D}{\longrightarrow} \mathcal N (0, \sigma^2)$,
\emph{then} $\sigma^2$ is the smallest possible among consistent estimators.

\subsubsection{Derivation}
We draw $X_1, X_2, \ldots, X_n$ i.i.d. sample from $f(x|\theta^*)$.
We are given the data, we do not know $\theta^*$.
We assume Regularity Conditions such as differentiability of 
$f(x|\theta)$ with respect to $\theta$.
Examine the Likelihood Function,
\[
L_n(\theta) = f(x_1|\theta) f(x_2|\theta) \ldots f(x_n|\theta),
\]
where $\prod_{i=1}^nf(x_i|\theta)$ denotes the joint density of 
$x_i$ drawn i.i.d. from $f(x|\theta)$. Let 
\[
\hat \theta_n = \argmax_{\theta \in \Theta} L_n(\theta).
\]

Notice that $\log$ is a monotone transform, and $f(x_j)$ non-negative,
hence $\prod_{k=1}^n f(x_k)$ non-negative. Then,

\[
\ell_n(\theta) = \frac{1}{n} \log L_n (\theta) = \frac{1}{n} \sum_{j=1}^n \log f(x_j|\theta).
\]

By WLLN,
\[
\ell_n(\theta) \overset{P}{\longrightarrow} \int_{-\infty}^\infty \log f(x|\theta) f(x|\theta^*) \D x,
\]
i.e. as $n \uparrow \infty$, $\Pr(|\ell_n (\theta) - \ell(\theta)| > \delta) \longrightarrow 0$ for all $\delta > 0$ and for each $\theta \in \Theta$.

In the section which follows, $f'$ always denotes derivative with 
respect to $\theta$.

\begin{alignat*}{5}
&\ell(\theta) &&= \int \log f(x|\theta) \cdot f(x|\theta^*) \D x \implies \ell '(\theta) = \int_{-\infty}^\infty \frac{f'(x|\theta)}{f(x|\theta)} f(x|\theta^*) \D x \hspace{25pt} &&\textrm{by chain rule} \\
\implies &\ell '(\theta^*) &&= \int_{-\infty}^\infty f'(x|\theta^*) \D x = \frac{\D}{\D \theta} \left( \int_{-\infty}^\infty f(x|\theta) \D x \right) \bigg|_{\theta = \theta^*} = 0. \end{alignat*}

Since $\ell'(\theta^*) = 0$, at least we know that $\theta^*$ is a 
critical point of $\ell(\theta)$.
We know that $\ell_n(\theta) \overset{P}{\longrightarrow} \ell(\theta)$ by WLLN.
Further, \href{http://www.amazon.com/Course-Sample-Chapman-Statistical-Science/dp/0412043718}{Ferguson (1996)}, this holds uniformly in $\theta$ for
\[
| \theta - \theta^*| \leq a.
\]
Hence for all $\delta > 0$,
\[
\Pr\left( \max_{|\theta - \theta^*| \leq a} \bigg|\ell_n(\theta) - \ell(\theta)\bigg| > \delta \right) \overset{n \uparrow \infty}{\longrightarrow} 0.
\]

Does this imply that $\hat \theta_n = \argmax_{|\theta - \theta^*| \leq a} \ell_n(\theta) \overset{P}{\longrightarrow} \theta^*$?
Yes! \href{http://projecteuclid.org/euclid.aoms/1177729952}{Wald (1948)}.

\subsubsection{Applying MLE in R}
Let's see how we can apply what we've learned in our favorite programming language, R!

<<>>=

### For use of the `%>%` operator.
require(magrittr)

### Simply a normal density. Also built-in to R: see dnorm.
MyDensity <- function(x, mu = 0, ss = 1) 
    1/sqrt(2*pi*ss) * exp(-(mu-x)^2/(2*ss))

### Returns minus the log-likelihood of a normally distributed RV.
LL <- function(beta, response, data)
    -1 * MyDensity(response - (data %>% as.matrix) %*% beta) %>% log %>% sum

@ 

\paragraph{Simulating Data}
Now that we've defined our functions, let's simulate some data.

<<>>=

N <- 10
P <- 2
X <- matrix(rnorm(P*N), nrow = N)
Y <- runif(N)

@ 

\paragraph{Running a regression}
Great, now we run a regression and compare the results.

<<>>=

m <- optim(par = c('beta' = rep(0, ncol(X))), fn = LL, response = Y, data = X)

@ 

\paragraph{Determining Correctness}
We verify that our regression well set-up.

<<>>=
m$par - lm(Y ~ 0 + X)$coef < 1e-3
@ 

\section{Bootstrapping confidence intervals}

\begin{itemize}   \item Our sample gives a sense of the population.
  \item In reality, we only have one sample and we don't get to see the entire population.
  \item As a best guess for the population, we can use the sample we actually observed. \end{itemize}


<<cache=T>>=
require(MASS)  # For dataset: Boston.
N <- nrow(Boston)

### Run one model.
model <- lm(crim ~ tax + medv, data = Boston)

### Run a series of models, each time bootstrapping a coefficient.
coefs <- replicate(1e4, expr = {
    lm(crim ~ tax + medv, data = Boston[sample(1:N, replace = T), ])$coef['tax']
})

@

With our statistics in hand, we're ready to bootstrap an estimated confidence interval for one of our coefficients.

<<echo=F>>=

my <- quantile(coefs, probs = c(0.025, 0.975))
ci <- confint(model, parm = 'tax')

hist(coefs, prob = T, main = 'Histogram of coefficients', xlab = 'Estimated coefficient')
abline(v = ci[1], lty = 'dashed', col = 'purple')
abline(v = ci[2], lty = 'dashed', col = 'purple')
abline(v = my[1], lty = 'dashed', col = 'orange')
abline(v = my[2], lty = 'dashed', col = 'orange')
legend(x = 0.0325, y = 100, col = c('purple', 'orange'), lty = 'dashed', 
       legend = c('R', 'Bootstrap'))

@ 

\section{Permutation Testing}
Let's examine whether there's a relationship between \texttt{distance} traveled
and \texttt{speed} in our \texttt{Cars} dataset.

<<>>=

tru.model <- lm(speed ~ dist, data = cars)

stats <- replicate(1e4, expr = {
    prmtd <- sample(1:nrow(cars), replace = F)
    lm(speed ~ I(dist[prmtd]), data = cars)$coef
})
stats <- stats[2, ]  # Fix attention to slope-coefficient only.

### Estimated p-value of observing data at least as extreme.
mean(abs(stats) >= abs(tru.model$coef['dist']))

@ 

\section{Statistical Inference}
Lastly, we investigate the cornerstone of statistical inference: the hypothesis test. \subsection{Hypothesis testing} \subsection{Confidence intervals}

\section{Homework/Practice}
\section{Problem \#0} 
%% Write an R function which samples the elements from the following  table with
%% their corresponding probabilities.
%% \begin{table}
%%   & A & B & C \\
%%   \textrm{Probability} & 0.4 & 0.3 & 0.1
%% \end{table}

<<>>=

MySample <- function(tbl, n) {
    replicate(n, expr = {
        cums <- cumsum(tbl)
        coin <- runif(1)
        if (all(coin < cums)) return(tbl[1])
        if (all(coin > cums)) return (tbl[length(tbl)])
        names(cums)[which.min(coin > cums)]
    })
}

MySample(tbl = c("A" = 0.45, "B" = 0.25, "C" = 0.35), n = 10)

@ 

 \section*{Problem \#1}  \paragraph{Question} A target is made of 3 concentric circles of radii $1/\sqrt{3}$, 1, and $\sqrt{3}$ feet. Shots within the inner circle are given 4 points, shots within the next ring are given 3 points, and shots within the third ring are given 2 points. Shouts outside the target are given 0 points.  Let $X$ denote the distance of the hit from the center (in feet), and let the p.d.f. of $X$ be  \[ f(x) = \frac{2}{\pi (1 + x^2)} \boldsymbol{1}\{x > 0\} \]  What is the expected value of the score of a single shot?  \paragraph{Solution} Using the definition of the expectation of a discrete-valued random variable, we have that  \begin{align*} \mathbb E[\text{single shot}] &= \sum_{x \in X} x \cdot \Pr(X = x) \\  &= 0 \cdot \Pr(X > \sqrt{3}) + 2 \cdot \Pr(1 < X < \sqrt{3}) + 3 \cdot \Pr(\frac{1}{\sqrt{2}} < X < 1) + 4 \cdot \Pr(X < \frac{1}{\sqrt{3}}) \end{align*}  The solution is simple once we solve for the c.d.f. of our random variable $X$. By definition,  \[ F(x) = \int_0^x f(x) dx = \int_0^x \frac{2}{\pi(1 + x^2)} dx \]  Recall that when $\sin \theta$ and $\cos \theta$ are defined using a unit circle, the $(x,y)$ point which lay on the unit circle is given by $(\cos \theta, \sin \theta)$, where $\theta$ denotes the angle from the origin.  Therefore, by the Pythagorean theorem, we have the identity that $\sin^2 \theta + \cos^2 \theta = 1$, since we can describe any point on the unit circle by a (possible degenerate) right triangle. We may then derive that  \begin{align*} \sin^2 \theta + \cos^2 \theta &= 1 \\ \frac{\sin^2 \theta}{\cos^2 \theta} + \frac{\cos^2\theta}{\cos^2 \theta} &= \frac{1}{\cos^2 \theta} \\ \tan^2 \theta + 1 &= \sec^2 \theta \end{align*}  This informs how we may use $u$-substitution to solve for our c.d.f. Let $x = \tan \theta$. Then,  \begin{align*} \frac{d}{d \theta} \tan \theta &= \frac{d}{d \theta} \frac{\sin \theta}{\cos \theta} \\  &= \frac{d}{d \theta} \sin \theta \cos^{-1} \theta \\  &= \frac{\cos \theta}{\cos \theta} + \frac{-\sin \theta}{\cos^2 \theta} \cdot (-\sin \theta) \\  &= \frac{\cos^2 \theta + \sin^2 \theta}{\cos^2 \theta} \\ &= \frac{1}{\cos^2 \theta} \\ &= \sec^2 \theta \end{align*}  If $x = \tan \theta$, then $\theta = \arctan x$, and further  \[ \frac{dx}{d\theta} = \sec^2 \theta \implies dx = \sec^2 \theta d \theta. \]  This is great because we can use $u$-substitution to reduce our integral to something very simple:  \[ \int \frac{1}{1 + x^2}dx = \int \frac{1}{\sec^2 \theta} \sec^2 \theta d\theta = \int d \theta = \theta + C \]  Therefore,  \begin{align*} \frac{2}{\pi} \int_0^x  \frac{1}{1+x^2}dx &= \frac{2}{\pi} \arctan x \bigg |_0^x = \frac{2}{\pi} \arctan x \end{align*}  Note that $\arctan 0 = 0$ and  $\underset{x \to \infty}{\lim} \arctan x \to \pi/2$, so that  $F(0) = 0$ and $\underset{x \to \infty}{\lim} F(x) \to 1$, which is how we expect our c.d.f. to behave for this random variable.  Now,  \begin{align*} \mathbb{E}[\text{single shot}] &= \sum_{x \in X} x \cdot \Pr(X = x) \\  &= 0 \cdot \Pr(X > \sqrt{3}) + 2 \cdot \Pr(1 < X < \sqrt{3}) + 3 \cdot \Pr(\frac{1}{\sqrt{2}} < X < 1) + 4 \cdot \Pr(X < \frac{1}{\sqrt{3}}) \\ &= 2 \bigg( F(\sqrt{3}) - F(1) \bigg) + 3 \bigg(F(1) - F(1/\sqrt{3}) \bigg) + 4 F(1/\sqrt{3}) \\ &\approx 3.403  \end{align*}  \section*{Problem \#2}   \paragraph{Question} Assume that the random variable $X$ has the exponential distribution  \[ f(x|\theta) = \theta e^{-\theta x} \boldsymbol{1}\{x > 0, \theta > 0\} \]  where $\theta$ is the parameter of the distribution. Use the method of maximum likelihood to estimate $\theta$ is 5 observations of $X$ are $x_1 = 0.9$, $x_2 = 1.7$, $x_3 = 0.4$, $x_4 = 0.3$, and $x_5 = 2.4$ are generated independent and identically distributed.  \paragraph{Answer} We are told that  \[ x_1, \ldots, x_n \overset{i.i.d.}{\sim} \textrm{exp} (\theta) \]  By independence, we have that  \[ f(x_1, \ldots, x_n | \theta_1, \ldots, \theta_n) = \prod_{i=1}^n f(x_i|\theta_i) \]  and by identically distributed, we have that  \[ \prod_{i=1}^n f(x_i) = \theta e^{-\theta x_i} \]  Therefore,  \[ f(x_1, \ldots, x_n|\theta) = \prod_{i=1}^n \theta e^{- \theta x_i} = \theta^n e^{-\theta \sum_i x_i} \]  Our likelihood function is given by  \[ L(\theta|x_1, \ldots, x_n) = \theta^n e^{-\theta \sum_i x_i}. \]  Maximizing a monotonic increasing transformation of the likelihood is the same as maximizing the likelihood, so to make life easier we take the log-likelihood:  \[ \lg L(\theta|x_1, \ldots, x_n) = n \lg \theta - \theta \sum_i x_i \]  Maximizing this by taking the derivative and setting it to zero shows that   \begin{align*} \frac{d}{d \theta} \lg L(\theta |x_1, \ldots, x_n) = \frac{n}{\theta} - \sum_i x_i &= 0 \\ \frac{n}{\theta} &= \sum_i x_i \\ \implies \hat{\theta}_{\text{mle}} &= \frac{n}{\sum_i x_i} \end{align*}  In other words, our maximum likelihood estimator for our parameter $\theta$ is simply the reciprocal of the sample mean. Note that we can convince ourselves that we have found a true maximum by taking the second derivative of our likelihood function and checking that it is concave, yielding  $\frac{d}{d \theta} \frac{n}{\theta} - \sum_i x_i = \frac{-n}{\theta^2}$, which is an upside down parabola and therefore has a single maximum point.  If $x = (0.9, 1.7, 0.4, 0.3, 2.4)$, then $\sum_i x_i = 5.7$, and $\bar{x} = 57/50 = 1.14$. Therefore, we see that  \[ \hat{\theta}_{\text{mle}} = \frac{1}{\bar{x}} = \frac{1}{1.14} \approx 0.877. \] 

\bibliographystyle{plain}
\begin{thebibliography}{10}
\bibitem{clain: ec2120}
G. Chamberlain.
\newblock {\em Econometrics 2120}.
\newblock Harvard University, Winter 2017.

\bibitem{papa: 308}
G. Papanicalou.
\newblock {\em CME 308: Stochastic Processes}.
\newblock Stanford University, Spring 2016.

\bibitem{ferguson: lst}
Thomas S. Ferguson
\newblock {\em A course in large sample theory}.
\newblock University of California, Los Angeles.

\bibitem{Wald: 48}
Abraham Wald.
\newblock {\em Note on the consistency of the maximum likelihood estimate}.
\newblock Annals of Mathematical Statistics, 1949.

\bibitem{islr: intro stat learning}
James et. al.
\newblock {\em An introduction to statistical learning}.
\newblock Springer 2017.


\bibitem{bstr: bstrapefron}
Bradley Efron.
\newblock {\em Bootstrap methods: another look at the jackknife}.
\newblock Annals of Statistics, 1979.

\bibitem{fin: finetti}
Bruno de Finetti.
\newblock {\em Theory of Probability}.
\newblock 1970, Wiley.

\end{thebibliography}

% Bstrapping prediction error?
% Handling 0-1 response variables
% A simple logistic regression?
% Handling categorical variables?

% Stats 305 notes section 9.6 (distributional results of OLS estimators)

\end{document}
